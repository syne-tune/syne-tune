{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8892acbd",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for fine-tuning pre-trained Transformer Models from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6973c2",
   "metadata": {},
   "source": [
    "In this notebook, we will show how we can use Syne Tune to optimize the hyperparameters of pre-trained transformers\n",
    "from Hugging Face when we fine tune them on NLP datasets from the GLUE benchmark suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72cb25",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbd432",
   "metadata": {},
   "source": [
    "Install Syne Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'syne-tune[extra]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebca81",
   "metadata": {},
   "source": [
    "Install HuggingFace dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea804c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'torch==1.10.0' 'datasets==1.8.0' 'transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl #$; mpl.use('pgf')\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from pathlib import Path\n",
    "\n",
    "from syne_tune.backend.local_backend import LocalBackend\n",
    "from syne_tune.tuner import Tuner\n",
    "from syne_tune.search_space import uniform, loguniform, choice, randint\n",
    "from syne_tune.stopping_criterion import StoppingCriterion\n",
    "from syne_tune.optimizer.baselines import ASHA, MOBSTER, BayesianOptimization, RandomSearch, MOASHA\n",
    "from syne_tune.constants import ST_WORKER_TIME\n",
    "from syne_tune.backend.sagemaker_backend.instance_info import select_instance_type\n",
    "from syne_tune.backend.sagemaker_backend.sagemaker_backend import SagemakerBackend\n",
    "from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role\n",
    "\n",
    "\n",
    "TASK2METRICSMODE = {\n",
    "    \"cola\": {'metric': 'matthews_correlation', 'mode': 'max'},\n",
    "    \"mnli\": {'metric': 'accuracy', 'mode': 'max'},\n",
    "    \"mrpc\": {'metric': 'f1', 'mode': 'max'},\n",
    "    \"qnli\": {'metric': 'accuracy', 'mode': 'max'},\n",
    "    \"qqp\": {'metric': 'f1', 'mode': 'max'},\n",
    "    \"rte\": {'metric': 'accuracy', 'mode': 'max'},\n",
    "    \"sst2\": {'metric': 'accuracy', 'mode': 'max'},\n",
    "    \"stsb\": {'metric': 'spearmanr', 'mode': 'max'},\n",
    "    \"wnli\": {'metric': 'accuracy', 'mode': 'max'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984fec4",
   "metadata": {},
   "source": [
    "## Optimizing Training Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690ad80",
   "metadata": {},
   "source": [
    "We start by optimizing hyperparameters that control the training process: learning rate, batch size and the ratio of training steps for warming up the learning rate, i.e where the learning rate is increased to its final value before we start decaying it back to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684484e0",
   "metadata": {},
   "source": [
    "We first need to define some additional parameters such as the dataset, the total number of runtime dedicated to the HPO process and the number of workers to distributed the evaluation of hyperparameters, etc. \n",
    "We set the number of workers equal to the number of GPUs, such that each GPU evaluates one configuration at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mrpc'\n",
    "max_runtime = 1800\n",
    "num_train_epochs = 3 \n",
    "n_workers = 1\n",
    "seed = 12345\n",
    "model_type = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f435ab",
   "metadata": {},
   "source": [
    "For Syne Tune we need to define which metric we would like to optimize, and whether we maximize or minimize this metirc. Additionally, we also specify the resource attribute that determines how much resources we spend for the evaluation of a hyperparameter configuration, e.g the number of epochs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed03fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'eval_' + TASK2METRICSMODE[dataset]['metric']\n",
    "mode = TASK2METRICSMODE[dataset]['mode']\n",
    "resource_attribute = 'epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3decaf6",
   "metadata": {},
   "source": [
    "Next we define the path to the training scripts that gets the hyperparameters as input argument and reports the above define metrics back to Syne Tune. Here we use an adated version of the original Hugging Face script: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = \"./run_glue.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b3a5a",
   "metadata": {},
   "source": [
    "The first configuration that we will always evaluate is the default configuration provided by Hugging Face. This makes sure that the configuration returned by Syne Tune performs at least as well as the default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d55152",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'per_device_train_batch_size': 32,\n",
    "    'warmup_ratio': 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad27c9",
   "metadata": {},
   "source": [
    "Now we can define the configuration space, which contains the hyperparameters that we want to optimize plus all other parameters that we want to pass to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = {\n",
    "    'learning_rate': loguniform(1e-6, 1e-4),\n",
    "    'per_device_train_batch_size': randint(16, 48),\n",
    "    'warmup_ratio': uniform(0, 0.5),\n",
    "    'num_train_epochs': num_train_epochs,\n",
    "    'model_name_or_path': model_type,\n",
    "    'task_name': dataset,\n",
    "    'do_train': True,\n",
    "    'max_seq_length': 128,\n",
    "    'seed': seed,\n",
    "    'output_dir': 'tmp/' + dataset,\n",
    "    'evaluation_strategy': 'epoch', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b56fe",
   "metadata": {},
   "source": [
    "Before we can start the optimization process, we have to define the backend. Here we use the LocalBackend, which distributes the HPO process on the local machine. Later we will also see how we can parallelize the search across a set of SageMaker training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee113f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = LocalBackend(entry_point=entry_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46936e2",
   "metadata": {},
   "source": [
    "We use ASHA (asynchronous successive halving) as HPO method, which samples configurations randomly and terminates poorly performing configurations early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHA(config_space, metric=metric,\n",
    "                 resource_attr=resource_attribute, max_t=num_train_epochs,\n",
    "                 mode=mode, random_seed=seed, points_to_evaluate=[default])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2442f1",
   "metadata": {},
   "source": [
    "Now we have everything together to start the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f869cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_criterion = StoppingCriterion(max_wallclock_time=max_runtime)\n",
    "tuner = Tuner(\n",
    "    backend=backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=n_workers,\n",
    ")\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e035ef",
   "metadata": {},
   "source": [
    "Below we plot the test error of the best found model over time. As reference we plot the learning curve (red dots) of the default configuration, which, as explained above, is always evaluated first.\n",
    "We can see that Syne Tune quickly finds a better performing configurations that improves upon the default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syne_tune.experiments import load_experiment\n",
    "exp = load_experiment(tuner.name)\n",
    "df_hyperparameters = exp.results\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "\n",
    "traj = df_hyperparameters['test_accuracy'].cummax()\n",
    "runtime = df_hyperparameters['st_tuner_time']\n",
    "plt.plot(runtime, traj)\n",
    "\n",
    "df_default = df_hyperparameters[df_hyperparameters['trial_id'] == 0]\n",
    "traj = df_default['test_accuracy'].cummax()\n",
    "runtime = df_default['st_tuner_time']\n",
    "plt.plot(runtime, traj, 'ro')\n",
    "\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('wall-clock time (seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9ef3b",
   "metadata": {},
   "source": [
    "## Optimize the choice of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3323f9f4",
   "metadata": {},
   "source": [
    "We extend the HPO such that it also automatically selected the right pre-trained model for us. We can achieve this by adding an additional categorical hyperparameter to our search space, that encodes the choice of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b004037",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space['model_name_or_path'] = choice(['bert-base-cased', 'bert-base-uncased', 'distilbert-base-uncased',\n",
    "                                             'distilbert-base-cased', 'roberta-base',\n",
    "                                             'albert-base-v2', 'distilroberta-base',\n",
    "                                             'xlnet-base-cased', 'albert-base-v1',\n",
    "                                             ])\n",
    "\n",
    "default['model_name_or_path'] = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad5970",
   "metadata": {},
   "source": [
    "Now we run the search again under the same conditions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d72088",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = LocalBackend(entry_point=entry_point)\n",
    "scheduler = ASHA(config_space, metric=metric,\n",
    "                 resource_attr=resource_attribute, max_t=num_train_epochs,\n",
    "                 mode=mode, random_seed=seed, points_to_evaluate=[default])\n",
    "\n",
    "stop_criterion = StoppingCriterion(max_wallclock_time=max_runtime)\n",
    "tuner = Tuner(\n",
    "    trial_backend=backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=n_workers,\n",
    ")\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dac70e",
   "metadata": {},
   "source": [
    "After the search is finished, we compare the results to our previous results. We can see that optimizing in this augmented search space allows Syne Tune to find a much better performing hyperparameter configuration in the same amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syne_tune.experiments import load_experiment\n",
    "exp_model_choice = load_experiment(tuner.name)\n",
    "df_model_choice = exp_model_choice.results\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "\n",
    "\n",
    "traj = df_hyperparameters['test_accuracy'].cummax()\n",
    "runtime = df_hyperparameters['st_tuner_time']\n",
    "plt.plot(runtime, traj, label='Hyperparameters not including model type')\n",
    "\n",
    "traj = df_model_choice['test_accuracy'].cummax()\n",
    "runtime = df_model_choice['st_tuner_time']\n",
    "plt.plot(runtime, traj, label='Hyperparameters including model type')\n",
    "\n",
    "df_default = df_model_choice[df_model_choice['trial_id'] == 0]\n",
    "traj = df_default['test_accuracy'].cummax()\n",
    "runtime = df_default['st_tuner_time']\n",
    "plt.plot(runtime, traj, 'ro')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('wall-clock time (seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba65e3",
   "metadata": {},
   "source": [
    "## Optimize the Instance Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102aeae",
   "metadata": {},
   "source": [
    "Last, we additionally automate the selection of the instance type for deploying the model later. While this will not affect predictive performance, the instance type influences the latency and the cost of training the model.\n",
    "Thus, we do not optimize a single objective anymore but multiple objectives at the same time. The results is not a single configuration but a set of configuratios that optimially trade-off one metric vs the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d075a",
   "metadata": {},
   "source": [
    "The same as with the model choice, we will add an additional categorical hyperparameter that determines the instance type. For this example, we have to use the SageMaker backend of Syne Tune, which evaluates each hyperparameter configuration in a single SageMaker Training Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d524e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "instance_types = ['ml.g4dn.xlarge', 'ml.g4dn.2xlarge', 'ml.p2.xlarge', 'ml.g4dn.4xlarge',\n",
    "                  'ml.g4dn.8xlarge', 'ml.p3.2xlarge']\n",
    "\n",
    "config_space['st_instance_type'] = choice(instance_types)\n",
    "\n",
    "backend = SagemakerBackend(\n",
    "    sm_estimator=HuggingFace(\n",
    "        entry_point=str('run_glue.py'),\n",
    "        source_dir=os.getcwd(),\n",
    "        base_job_name='glue-moasha',\n",
    "        # instance-type given here are override by Syne Tune with values sampled from `st_instance_type`.\n",
    "        instance_type='ml.m5.large',\n",
    "        instance_count=1,\n",
    "        py_version=\"py38\",\n",
    "        pytorch_version='1.9',\n",
    "        transformers_version='4.12',\n",
    "        max_run=3600,\n",
    "        role=get_execution_role(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c11089",
   "metadata": {},
   "source": [
    "Instead of single-objective ASHA, we now use MO-ASHA to optimize multiple objective simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syne_tune.constants import ST_WORKER_COST\n",
    "\n",
    "scheduler = MOASHA(\n",
    "    max_t=num_train_epochs,\n",
    "    time_attr=resource_attribute,\n",
    "    metrics=[metric, 'latency', ST_WORKER_COST],\n",
    "    mode=[mode, 'min', 'min'],\n",
    "    config_space=config_space,\n",
    ")\n",
    "stop_criterion = StoppingCriterion(max_wallclock_time=10800)\n",
    "tuner = Tuner(\n",
    "    backend=backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=4,\n",
    ")\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb70cf7",
   "metadata": {},
   "source": [
    "Below we plot the latency and cost of all configurations that we trained for the full amount of epochs. The color indicated the instance type. We also plot the Pareto front (dashed black line), i.e the set of configurations that dominate all other configurations in at least one objective and equally well in all other objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from syne_tune.constants import ST_WORKER_COST\n",
    "from syne_tune.experiments import load_experiment\n",
    "\n",
    "\n",
    "def is_pareto_efficient(costs, return_mask = True):\n",
    "    \"\"\"\n",
    "    Find the pareto-efficient points\n",
    "    :param costs: An (n_points, n_costs) array\n",
    "    :param return_mask: True to return a mask\n",
    "    :return: An array of indices of pareto-efficient points.\n",
    "        If return_mask is True, this will be an (n_points, ) boolean array\n",
    "        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n",
    "    \"\"\"\n",
    "    is_efficient = np.arange(costs.shape[0])\n",
    "    n_points = costs.shape[0]\n",
    "    next_point_index = 0  # Next index in the is_efficient array to search for\n",
    "    while next_point_index<len(costs):\n",
    "        nondominated_point_mask = np.any(costs<costs[next_point_index], axis=1)\n",
    "        nondominated_point_mask[next_point_index] = True\n",
    "        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n",
    "        costs = costs[nondominated_point_mask]\n",
    "        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n",
    "    if return_mask:\n",
    "        is_efficient_mask = np.zeros(n_points, dtype = bool)\n",
    "        is_efficient_mask[is_efficient] = True\n",
    "        return is_efficient_mask\n",
    "    else:\n",
    "        return is_efficient\n",
    "\n",
    "exp = load_experiment(tuner.name)\n",
    "df = exp.results\n",
    "metric_name = 'test_accuracy'\n",
    "\n",
    "df = df[df['epoch'] == 3]\n",
    "\n",
    "markers = ['o', 'D', 's', 'x', 'p', 'h']\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "\n",
    "x = df['latency']\n",
    "y = 1 - df[metric_name]\n",
    "\n",
    "data = np.array([x, y]).T\n",
    "idx = is_pareto_efficient(data)\n",
    "sort = np.argsort(data[idx, 0])\n",
    "plt.step(data[idx, 0][sort], data[idx, 1][sort], linestyle='--', color='k', where='post')\n",
    "\n",
    "for i, instance_type in enumerate(df.config_st_instance_type.unique()):\n",
    "    sub_df = df[df['config_st_instance_type'] == instance_type]\n",
    "    plot_label = True\n",
    "    for j, model_type in enumerate(df.config_model_name_or_path.unique()):\n",
    "        \n",
    "        sub_sub_df = sub_df[sub_df['config_model_name_or_path'] == model_type]\n",
    "        x = sub_sub_df['latency']\n",
    "        y = 1 - sub_sub_df[metric_name]\n",
    "        if plot_label:\n",
    "            plt.scatter(x, y, color='C%d' % i, marker='o', label=instance_type)\n",
    "            plot_label = False\n",
    "        else:\n",
    "            plt.scatter(x, y, color='C%d' % i, marker='o')\n",
    "\n",
    "plt.ylabel('test error')\n",
    "plt.xlabel('latency (milliseconds)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "\n",
    "x = df[ST_WORKER_COST]\n",
    "y = df['latency']\n",
    "\n",
    "data = np.array([x, y]).T\n",
    "idx = is_pareto_efficient(data)\n",
    "sort = np.argsort(data[idx, 0])\n",
    "plt.step(data[idx, 0][sort], data[idx, 1][sort], linestyle='--', color='k', where='post')\n",
    "\n",
    "for i, instance_type in enumerate(df.config_st_instance_type.unique()):\n",
    "    sub_df = df[df['config_st_instance_type'] == instance_type]\n",
    "    plot_label = True\n",
    "    for j, model_type in enumerate(df.config_model_name_or_path.unique()):\n",
    "        \n",
    "        sub_sub_df = sub_df[sub_df['config_model_name_or_path'] == model_type]\n",
    "        x = sub_sub_df[ST_WORKER_COST]\n",
    "        y = sub_sub_df['latency']\n",
    "        \n",
    "        if plot_label:\n",
    "            plt.scatter(x, y, color='C%d' % i, marker='o', label=instance_type)\n",
    "            plot_label = False\n",
    "        else:\n",
    "            plt.scatter(x, y, color='C%d' % i, marker='o')\n",
    "\n",
    "plt.ylabel('latency')\n",
    "plt.xlabel('cost (Dollar)')\n",
    "plt.legend()\n",
    "plt.xlim(0.01, 0.12)\n",
    "plt.ylim(4, 15)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
